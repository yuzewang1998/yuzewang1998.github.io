<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yuze Wang</title>

  <meta name="author" content="Yuze Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table
    style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:60%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yuze Wang | 王宇泽</name>
                  </p>
                  <p>
                    I'm a second-year Ph.D. under the supervision of Prof. Yue Qi at the <a
                      href="https://vrlab.buaa.edu.cn/">State Key Laboratory of Virtual Reality Technology and
                      Systems</a>, School of Computer Science and Engineering, Beihang University.
                  </p>
                  <p>
                    My research interest are mainly in<b> 3D content generation (reconstruction), human-computer
                      interaction
                      , and scene understanding for VR/MR/AR. Especially using Internet-sourced data</b>. Feel free to
                    contact me for any questions, collaboration opportunities, or just for a chat.
                  </p>
                  <p style="text-align:center">
                    <a href="yuzewang1998@buaa.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href=https://yuzewang1998.github.io/">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=V_5VPiEAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="images/wechat.jpg">WeChat</a> &nbsp;/&nbsp;
                    <a href="https://github.com/yuzewang1998">Github</a>
                  </p>
                </td>
                <td style="padding:3%;width:40%;max-width:40%">
                  <img style="width:70%;max-width:70%" alt="profile photo" src="images/pandas.jpg"
                    class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>




          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p>
                    <heading>Preprints</heading>
                  </p>
                  <p>
                    * Indicates equal contribution ( equal first author corresponding autor)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/takinglangsplatw.png" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>Taking Language Embedded 3D Gaussian Splatting into the Wild
                      </papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>arxiv 2025.07 </em>
                      <br>
                      <a href="https://yuzewang1998.github.io/takinglangsplatw/">[Project Page]</a>
                      <a href=" ">[Paper]</a>
                      <a href="https://github.com/yuzewang1998/takinglangsplatw">[Code]</a>
                      <a href=" ">[Data]</a>
                      <br>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/deocc123.png" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>DeOcc-1-to-3: 3D De-Occlusion from a Single Image via Self-Supervised Multi-View
                        Diffusion</papertitle>
                      <br>
                      <a href="https://quyans.github.io/"> Yansong Qu </a>,
                      <a href=""> Shaohui Dai</a>,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=M9rwkHwAAAAJ"> Xinyang Li </a>,
                      <strong> Yuze Wang </strong>,
                      <a href=""> You Shen</a>,
                      <a href="https://scholar.google.com.hk/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan
                        Cao</a>,
                      <a href="https://scholar.google.com.hk/citations?user=lRSD7PQAAAAJ&hl=zh-CN&oi=ao"> Rongrong
                        Ji</a>
                      <br>
                      <em>arxiv 2025.06 </em>
                      <br>
                      <a href="https://quyans.github.io/DeOcc123/">[Project Page]</a>
                      <a href="https://arxiv.org/pdf/2506.21544">[Paper]</a>
                      <br>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/wegs_2024.gif" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>WE-GS: An In-the-wild Efficient 3D Gaussian Representation for Unconstrained Photo
                        Collections</papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="https://junyiwang.github.io/">Junyi Wang</a>,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>arxiv 2024.06 </em>
                      <br>
                      <a href="https://yuzewang1998.github.io/we-gs.github.io/">[Project Page]</a>
                      <a href="https://arxiv.org/abs/2406.02407">[Paper]</a>
                      <br>
                    </td>
                  </tr>

                </tbody>
              </table>



              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <p>
                        <heading>Selected Publications</heading>
                      </p>
                      <p>
                        * indicates equal contribution ( equal first autor or equal corresponding author)
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>


                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/seg_wild.png" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained
                        Image Collections</papertitle>
                      <br>
                      <a href="">Yongtang Bao</a>,
                      <a href="">Chengjie Tang</a>,
                      <strong>Yuze Wang*</strong>,
                      <a href="">Haojie Li* </a>
                      <br>
                      <em>ACM MM 2025 </em>
                      <br>
                      <a href="https://arxiv.org/pdf/2507.07395">[ArXiv]</a>
                      <a href="https://github.com/Sugar0725/Seg-Wild">[Code]</a>
                      <br>
                    </td>
                  </tr>




                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/rise_editing_2025.gif" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>RISE-Editing: Rotation-invariant Neural Point Fields with Interactive Segmentation for
                        Fine-grained and Efficient Editing</papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="https://junyiwang.github.io/">Junyi Wang</a>,
                      <a href="">Chen Wang</a>,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>Neural Networks 2025 </em>
                      <br>
                      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608025001832">[Paper]</a>
                      <a href="https://github.com/yuzewang1998/RISE-Editing">[Code]</a>
                      <br>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/lasgs_2025.gif" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>Look at the Sky: Sky-aware Efficient 3D Gaussian Splatting in the Wild</papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="https://junyiwang.github.io/">Junyi Wang</a>,
                      <a href="">Ruicheng Gao</a>,
                      <a href="https://quyans.github.io/">Yansong Qu</a>,
                      <a href="">Wantong Duan </a> ,
                      <a href="">Shuo Yang </a> ,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>IEEE Virtual Reality (VR) 2025 -- IEEE Transactions on Visualization and Computer Graphics
                        (TVCG) 2025 <p style='color:red'> <b>IEEE VR 2025 Best Paper Award </b> </p> </em>
                      <br>
                      <a href="https://ieeexplore.ieee.org/abstract/document/10916815">[Paper]</a>
                      <a href="https://www.youtube.com/watch?v=0ZVuUjg0ck8">[Video]</a>
                      <br>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/scarf_2024.jpg" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>SCARF: Scalable Continual Learning Framework for Memory‐efficient Multiple Neural
                        Radiance Fields</papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="https://junyiwang.github.io/">Junyi Wang</a>,
                      <a href="">Chen Wang</a>,
                      <a href="">Wantong Duan </a> ,
                      <a href="http://cise.sdust.edu.cn/home/Page/teacher_detail/catId/31/id/503.html">Yongtang Bao </a>
                      ,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>Pacific Graphics (PG) 2024 -- Computer Graphics Forum (CGF) 2024 </em>
                      <br>
                      <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.15255">[Paper]</a>
                      <br>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/sgnerf2.png" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>SG-NeRF: Semantic-guided Point-based Neural Radiance Fields</papertitle>
                      <br>
                      <a href="https://quyans.github.io/">Yansong Qu*</a>,
                      <strong>Yuze Wang*</strong>,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>ICME, 2023</em>
                      <br>
                      <a href="https://ieeexplore.ieee.org/abstract/document/10219715">[Paper]</a>
                      <a href="https://github.com/Quyans/SG-NeRF">[Code]</a>
                      <br>
                      <!-- <p> Neural Radiance Fields (NeRF) require a large number of high-quality images to achieve novel view synthesis in room-scale scenes, but capturing these images is very labor-intensive. To address this, we propose Semantic-Guided Point-Based NeRF (SG-NeRF), which can reconstruct the radiance field using only a few images. We leverage sparse 3D point clouds with neural features as geometry constraints for NeRF optimization and use semantic predictions from both 2D images and 3D point clouds to guide the search for neighboring neural points during ray marching. This semantic guidance allows the sampled points to accurately find structurally related points even in large areas with unevenly distributed sparse point clouds, enabling high-quality rendering with fewer input images. </p> -->
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/ripnerf_2023.gif" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>RIP-NeRF: Learning Rotation-Invariant Point-based Neural
                        Radiance Field for Fine-grained Editing and Compositing</papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="https://junyiwang.github.io/">Junyi Wang</a>,
                      <a href="https://quyans.github.io/">Yansong Qu</a>,
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=Q-UkdjUAAAAJ">Yue Qi </a>
                      <br>
                      <em>ICMR, 2023</em>
                      <br>
                      <a href="https://dl.acm.org/doi/abs/10.1145/3591106.3592276">[Paper]</a>
                      <a href="https://github.com/yuzewang1998/RISE-Editing">[Code]</a>
                      <br>
                      <!-- <p>In this work, we introduce Rotation-Invariant Point-based NeRF (RIP-NeRF), combining the strengths of implicit NeRF-based and explicit point-based representations for fine-grained editing and cross-scene compositing of radiance fields. We replace the traditional Cartesian coordinates with a novel rotation-invariant point-based radiance field representation, using a Neural Inverse Distance Weighting Interpolation (NIDWI) module to enhance rendering quality. For cross-scene compositing, we disentangle the rendering module from the neural point-based representation, allowing controllable compositing across scenes without the need for retraining.</p> -->
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/ieee_sensors_2019.jpg" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>An information entropy-based method of evidential source separation and refusion
                      </papertitle>
                      <br>
                      <strong>Yuze Wang</strong>,
                      <a href="">Jindong Zhang</a>,
                      <a href="">Jiale Qiao</a>
                      <br>
                      <em>IEEE Sensors Journal 2020 </em>
                      <br>
                      <a href="https://ieeexplore.ieee.org/abstract/document/8832186">[Paper]</a>
                      <br>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:30%;max-width:30%" align="center">
                      <img style="width:100%;max-width:100%" src="images/soft_computing_2020.jpg" alt="dise">
                    </td>
                    <td width="75%" valign="center">
                      <papertitle>An improved multi-sensor D–S rule for conflict reassignment of failure rate of set
                      </papertitle>
                      <br>
                      <a href="">Jiale Qiao</a>,
                      <a href="">Jindong Zhang</a>,
                      <strong>Yuze Wang</strong>
                      <br>
                      <em>Soft Computing 2019</em>
                      <br>
                      <a href="https://link.springer.com/article/10.1007/s00500-020-05298-5">[Paper]</a>
                      <br>
                    </td>
                  </tr>


                </tbody>
              </table>


              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Academic Services</heading>
                      <p>
                        <li style="margin: 5px;">
                          Review for <b>IEEE TVCG</b>, <b>IEEE VR 2024, 2025</b>, <b>CHI 2025</b>, <b>IEEE ISMAR
                            2025</b>, <b>Pacific Vis (TVCG Track) 2025</b>, etc.
                        </li>
                    </td>
                  </tr>
                </tbody>
              </table>

              <!--
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Honors and Awards</heading>
          <p>
             <li style="margin: 5px;"> <b>Beihang University First-Class Scholarship (Top 5% in 2024)</b></li>
          </p>
        </td>
      </tr>
    </tbody></table>
   -->
              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        <a href="https://quyans.github.io/">Website Template</a>
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>

  <p>
    <center>
      <div id="clustrmaps-widget" style="width:10%">
        <script type="text/javascript" id="clustrmaps"
          src="//clustrmaps.com/map_v2.js?d=pQlgj-atr-4jJJMpbbHZZ2pWohNtjpcXZPimllfH6ZY&cl=ffffff&w=a"></script>
        <br>
        <p>&copy; Yuze Wang</p>
        <p>Last updated: 24, Jan, 2025</p>
    </center>
  </p>
</body>

</html>